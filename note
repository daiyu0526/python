机器学习视频课程作业
1、单数据线性回归问题
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
pd.set_option('display.max_rows', None)#显示全部行
pd.set_option('display.max_columns', None)#显示全部列

path =  'ex1data1.txt'
data = pd.read_csv(path, header=None, names=['Population','Profit']) # 数据读入，读入函数来自pandas
data.head() # 显示数据列表前五行的数据加上print语句
data.describe() # 计算得到这组数据的统计数据的
data.plot(kind='scatter', x='Population',y='Profit', figsize=(12,8)) # 这条绘图的语句有点不懂
plt.show()   # 完成了数据导入和数据显示



def computeCost(X, y, theta): # 定义计算代价函数，这里X用的是大写，核心是想说明多种特征的输入
    inner = np.power(((X * theta.T) - y), 2) # 公式计算np.power，用inner表示在求和符号里面的数值
    # print(theta) 一维向量在转置中会有不变的情况，整体看完后可调试
    return np.sum(inner) / (2 * len(X))

data.insert(0, 'Ones', 1) # 在数据列表前面加了新的一列，名字是ones，值均为1，对应于课程中的theta0的取值

# set X (training data) and y (target variable) 这一部分就是对数据的再处理，原理就是老师课上讲的内容
cols = data.shape[1] # 求取修改后的data的列数
X = data.iloc[:,0:cols-1] # 这一步将X于y所在后续实验中的值都分离开来各自组成矩阵了
y = data.iloc[:,cols-1:cols] # X是所有行，最后一列
X.head() # head()是观察前5行
y.head()
X = np.matrix(X.values) # 构造形成矩阵形势，不再是前面简单的数据了
y = np.matrix(y.values) # 数据在进行输入到最终计算时都是变成数组形势
theta = np.matrix(np.array([0,0]))
theta
X.shape, theta.shape, y.shape

computeCost(X, y, theta) # 上述将数值设好，然后调用函数求得代价函数
fp = open("E:/资源代码/test1.text", "a+")  # a+ 如果文件不存在就创建。存在就在文件内容的后面继续追加
print(computeCost(X,y,theta), file=fp)    # 这里将代价函数的结果打印出来了
fp.close()


# 定义梯度下降函数
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape)) # 构建一个和前面的theta的大小一致的0矩阵作为temp，临时暂存不同的theta值
    parameters = int(theta.ravel().shape[1]) # 似是函数的叠用，选定的是theta的列，将其拉成一行，可以最后尝试输出一下
    cost = np.zeros(iters) # iters是迭代次数

    for i in range(iters): # 开始迭代循环
        error = (X * theta.T) - y       # *******************尝试像所给代码中列出每一次迭代后产生的theta和error等一些值
        fp = open("E:/资源代码/test1.text", "a+")  # a+ 如果文件不存在就创建。存在就在文件内容的后面继续追加
        print(error,"误差", file=fp)           # 从输出可以看出，error是一个数组，其每一个值对应的就是估计得到的数值减去实际值得到的结果
        fp.close()
        for j in range(parameters):    # j指的应该是特征数据的个数
            term = np.multiply(error, X[:, j]) # X[:, j]这个表示只适用于矩阵或者是数组
            fp = open("E:/资源代码/test1.text", "a+")  # a+ 如果文件不存在就创建。存在就在文件内容的后面继续追加
            print(term,"term", file=fp)                                  # term是求和号里面的数据(h(x)-y)*x
            fp.close()
            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term)) # temp是将这些求和得到的数据
            fp = open("E:/资源代码/test1.text", "a+")  # a+ 如果文件不存在就创建。存在就在文件内容的后面继续追加
            print(temp,"temp",file=fp)
            fp.close()
        fp = open("E:/资源代码/test1.text", "a+")  # a+ 如果文件不存在就创建。存在就在文件内容的后面继续追加
        print("\n", file=fp)
        fp.close()
        theta = temp # 每一轮都得更新
        cost[i] = computeCost(X, y, theta)

    return theta, cost

# 设定其运算中的计算率和迭代次数
alpha = 0.01
iters = 1000

g, cost = gradientDescent(X, y, theta, alpha, iters)
g
computeCost(X, y, g)


x = np.linspace(data.Population.min(), data.Population.max(), 100)
f = g[0, 0] + (g[0, 1] * x)

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Traning Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size')
plt.show()
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters), cost, 'r')
ax.set_xlabel('Iterations')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')
plt.show()







&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
2、多特征参数线性回归问题
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # 空间三维画图
import pandas as pd
pd.set_option('display.max_rows', None)#显示全部行
pd.set_option('display.max_columns', None)#显示全部列

# 数据读入以及绘制图像
path =  'ex1data3.txt'
data = pd.read_csv(path, header=None, names=['Population', 'Room number','Profit']) # 数据读入，读入函数来自pandas
data = (data - data.mean()) / data.std() # 这一步实际上是对数据进行了预处理，特征归一化得到的，如果不进行特征归一化，所得的数值在计算时就会溢出

def computeCost(X, y, theta): # 定义计算代价函数，这里X用的是大写，核心是想说明多种特征的输入
    inner = np.power(((X * theta.T) - y), 2) # 公式计算np.power，用inner表示在求和符号里面的数值
    # print(theta) 一维向量在转置中会有不变的情况，整体看完后可调试
    return np.sum(inner) / (2 * len(X))

data.insert(0, 'Ones', 1) # 在数据列表前面加了新的一列，名字是ones，值均为1，对应于课程中的theta0的取值
cols = data.shape[1] # 求取修改后的data的列数
X = data.iloc[:,0:cols-1] # 这一步将X于y所在后续实验中的值都分离开来各自组成矩阵了
y = data.iloc[:,cols-1:cols] # X是所有行，最后一列
X = np.matrix(X.values) # 构造形成矩阵形势，不再是前面简单的数据了
y = np.matrix(y.values) # 数据在进行输入到最终计算时都是变成数组形势
theta = np.matrix(np.array([0,0,0]))


computeCost(X, y, theta) # 上述将数值设好，然后调用函数求得代价函数
print(computeCost(X,y,theta))
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape)) # 构建一个和前面的theta的大小一致的0矩阵作为temp，临时暂存不同的theta值
    parameters = int(theta.ravel().shape[1]) # 似是函数的叠用，选定的是theta的列，将其拉成一行，可以最后尝试输出一下
    cost = np.zeros(iters) # iters是迭代次数

    for i in range(iters): # 开始迭代循环
        error = (X * theta.T) - y       # *******************尝试像所给代码中列出每一次迭代后产生的theta和error等一些值
        #fp = open("E:/资源代码/test1.text", "a+")  # a+ 如果文件不存在就创建。存在就在文件内容的后面继续追加
        #print(error,"误差", file=fp)           # 从输出可以看出，error是一个数组，其每一个值对应的就是估计得到的数值减去实际值得到的结果
        #fp.close()
        for j in range(parameters):    # j指的应该是特征数据的个数
            term = np.multiply(error, X[:, j]) # X[:, j]这个表示只适用于矩阵或者是数组
            #fp = open("E:/资源代码/test1.text", "a+")  # a+ 如果文件不存在就创建。存在就在文件内容的后面继续追加
            #print(term,"term", file=fp)                                  # term是求和号里面的数据(h(x)-y)*x
            #fp.close()
            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term)) # temp是将这些求和得到的数据
            #fp = open("E:/资源代码/test1.text", "a+")  # a+ 如果文件不存在就创建。存在就在文件内容的后面继续追加
            #print(temp,"temp",file=fp)
            #fp.close()
        #fp = open("E:/资源代码/test1.text", "a+")  # a+ 如果文件不存在就创建。存在就在文件内容的后面继续追加
        #print("\n", file=fp)
        #fp.close()
        theta = temp # 每一轮都得更新
        cost[i] = computeCost(X, y, theta)

    return theta, cost

# 设定其运算中的计算率和迭代次数
alpha = 0.01
iters = 1000

g, cost = gradientDescent(X, y, theta, alpha, iters)
g
computeCost(X, y, g)


x = np.linspace(data.Population.min(), data.Population.max(), 100)
f = g[0, 0] + (g[0, 1] * x)

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Traning Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size')
plt.show()
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters), cost, 'r')
ax.set_xlabel('Iterations')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')
plt.show()




&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
pd.set_option('display.max_rows', None)#显示全部行
pd.set_option('display.max_columns', None)#显示全部列

# 数据导入及分类
path = 'ex2data1.txt'
data = pd.read_csv(path, header=None, names=['Exam 1', 'Exam 2', 'Admitted'])
positive = data[data['Admitted'].isin([1])] # 这个isin函数用法：取data数据中admitted数据列中值为1的所有值
negative = data[data['Admitted'].isin([0])]

# 数据可视化
fig, ax = plt.subplots(figsize=(12,8)) # 画图前需要明确这个图形
ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')
ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')
ax.legend()
ax.set_xlabel('Exam 1 Score')
ax.set_ylabel('Exam 2 Score')
plt.show()

# 逻辑回归方程，分类问题中的逻辑回归算法和回归问题中的线性算法的假设函数是可以不一致的，假设函数是主要随着问题在变的
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
# 代价函数计算
def cost(theta, X, y):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y) # 这一步是把这些数值全部弄成矩阵的形式
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    return np.sum(first - second) / (len(X))

# add a ones column - this makes the matrix multiplication work out easier
data.insert(0, 'Ones', 1)

# set X (training data) and y (target variable)
cols = data.shape[1]
X = data.iloc[:,0:cols-1]
y = data.iloc[:,cols-1:cols]

# convert to numpy arrays and initalize the parameter array theta
X = np.array(X.values)
y = np.array(y.values)
theta = np.zeros(3)

# 仅计算了梯度步长，梯度步长也就是将求导得到的那一部分
def gradient(theta, X, y):
    theta = np.matrix(theta)  # theta设成为了一个矩阵形式，同理也在x与y中
    X = np.matrix(X)
    y = np.matrix(y)

    parameters = int(theta.ravel().shape[1])  # 参数同样也是将其拉展成为一行数据进行相成
    grad = np.zeros(parameters) # 初始值时都是设这个数值为0

    error = sigmoid(X * theta.T) - y

    for i in range(parameters):
        term = np.multiply(error, X[:, i])
        grad[i] = np.sum(term) / len(X)

    return grad

gradient(theta, X, y)

# 使用TNC寻找最优参数，这个函数自己有自己的一套计算方法的，在使用时可以查找其相应的使用参数信息
import scipy.optimize as opt
result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))

cost(result[0], X, y)

# 也要写出预测函数，输入进去一个x需要判断期结果
def predict(theta, X):
    probability = sigmoid(X * theta.T)
    return [1 if x >= 0.5 else 0 for x in probability]

theta_min = np.matrix(result[0])
predictions = predict(theta_min, X)
correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]
accuracy = (sum(map(int, correct)) % len(correct))
print ('accuracy = {0}%'.format(accuracy))

# support标签中出现的次数
# precision查准率，recall召回率，f1-score调和平均数
from sklearn.metrics import classification_report
print(classification_report(y, predictions))

coef = -result[0] / result[0][2]
x = np.arange(30, 100, 0.5)
y = coef[0] + coef[1] * x

fig, ax = plt.subplots(figsize=(12, 8))
ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')
ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')
ax.plot(x, y, label='Decision Boundary', c='grey')
ax.legend()
ax.set_xlabel('Exam 1 Score')
ax.set_ylabel('Exam 2 Score')
plt.show()
